# -*- coding: utf-8 -*-
"""Snippets: Importing libraries

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WpNE1_edsfqcFcIgw27rpQg6h_Z9dMDj?usp=sharing
"""

import time
import numpy as np
from cvxopt import matrix, solvers
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from load_data import load_data
from sklearn.model_selection import train_test_split
import seaborn as sns
from sklearn import preprocessing
from sklearn.svm import SVC 
from sklearn.preprocessing import StandardScaler

def predict(X,w,b):
    y=[]
    for x in X:
        tmp = np.dot(x,w) + b
        if tmp>0:
            y.append(1)
        else:
            y.append(-1)
    return np.array(y)

X,y = load_data('iris.txt')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

plt.plot(X_train[np.where(y_train==1)[0], 0], X_train[np.where(y_train==1)[0], 1], 'rx')
plt.plot(X_train[np.where(y_train==-1)[0], 0], X_train[np.where(y_train==-1)[0], 1], 'bo')
plt.xlabel('petal_length')
plt.ylabel('petal_width')
plt.show()

C = 0.1
m,n = X_train.shape
P = np.zeros((m,m))
for i in range(m):
    for j in range(m):
        if i == j:
            P[i,j] = y_train[i,:] * y_train[j,:] * np.dot(X_train[i, :].T, X_train[j,:])
            continue
        P[i,j] = 0.5 * y_train[i,:] * y_train[j,:] * np.dot(X_train[i, :].T, X_train[j,:])
P = matrix(P)

q = -np.ones((m,1))
q = matrix(q,(m,1), 'd')

g1 = np.eye(m)
g2 = -np.eye(m)
g = np.concatenate((g1, g2), axis = 0)
G = matrix(g)

h1 = C * np.ones((m,1))
h2 = np.zeros((m,1))
h = np.concatenate((h1, h2), axis = 0)
h = matrix(h)
A = matrix(y_train,(y_train.T.shape), 'd')
b = matrix(0.0)

sol=solvers.qp(P, q, G, h, A, b)
print(sol['x'])

alpha = np.array(sol['x'])
w = np.sum(alpha * y_train * X_train, axis=0)
bias = np.mean(y_train - np.dot(X_train, w.T))

print('Final hyper plane parameters - ')
print('W: ', w.T, 'and b:', bias)
print('Number of support vectors: ', len(alpha))

plt.plot(X_test[np.where(y_test==1)[0], 0], X_test[np.where(y_test==1)[0], 1], 'rx')
plt.plot(X_test[np.where(y_test==-1)[0], 0], X_test[np.where(y_test==-1)[0], 1], 'bo')
x1, x2 = np.min(X[:,1]), np.max(X[:,1])
y1 = -w[0]/w[1] * x1 - bias/w[1]
y2 = -w[0]/w[1] * x2 - bias/w[1]
plt.plot([x1, x2], [y1,y2], color='green')
plt.xlabel('petal_length')
plt.ylabel('petal_width')
plt.show()

y_pred=predict(X_test,w,b)
print(y_pred)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
genralization_error=1-accuracy
print(genralization_error)

w_norm = np.linalg.norm(w)
support_vectors = []
comparison_value = 0.3
for i, x in enumerate(X):
  # Calculate the distance of x from the decision boundary
  distance = abs(w.dot(x) + bias) / w_norm
  # If x is a support vector, add it to the list
  if distance < comparison_value:
    support_vectors.append(x)
# Calculate the number of support vectors
vc = len(support_vectors)
print(vc)